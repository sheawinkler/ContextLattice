# Pilot Landing Page Copy (v1)

## Headline
Fix context drift before it burns your token budget.

## Subhead
Local-first memory fabric for AI agents with HTTP MCP, federated retrieval, durable fanout, and automatic queue protection.

## What You Get (2-4 week pilot)
- Baseline measurement for token spend, long-context dependence, and failure rate.
- Deployment of ContextLattice with retrieval + write-fanout tuned to your workflow.
- Post-pilot ROI summary with rollout recommendation.

## What Is New In This Build
- Fanout coalescer window to collapse duplicate hot writes.
- Letta backlog admission control to protect throughput during bursts.
- Sink retention worker for low-value cleanup in Qdrant/Letta (Mongo optional).
- Shared HTTP pools and batched fanout writes for lower overhead.

## Who It Is For
- LLM platform teams
- AI infrastructure teams
- IDE and agent product teams

## Why Now
- Long-context spend scales unpredictably under multi-agent workloads.
- Reliability degradation often starts in memory fanout and retrieval drift.
- Durable, multi-sink memory with queue guardrails improves both cost and completion quality.

## CTA
Book a 20-minute call for a context-cost audit and rollout plan.

## Trust Signals
- Local-first by default
- BYOK-compatible deployment posture
- Retention and audit controls
